{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5回演習課題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNISTデータの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_x, mnist_y = mnist.data.astype(\"float32\")/255.0, mnist.target.astype(\"int32\")\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(mnist_x, mnist_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題1：Denoising auto-encoderの実装．また，MNISTを用いて次のことを確認．\n",
    "* reconstruction errorが小さくなっている（学習が進んでいる）．\n",
    "* 重みの可視化（特徴の可視化）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1234)\n",
    "theano_rng = RandomStreams(rng.randint(1234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(params,gparams,lr=0.1):\n",
    "    updates = OrderedDict()\n",
    "    for param, gparam in zip(params, gparams):\n",
    "        updates[param] = param - lr * gparam\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoderクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "    def __init__(self,visible_dim,hidden_dim,function):\n",
    "        ## WRITE ME\n",
    "        self.a = \n",
    "        self.b = \n",
    "        self.params = []\n",
    "        \n",
    "    #encoder\n",
    "    def encode(self,x):\n",
    "        ## WRITE ME\n",
    "        return y\n",
    "    \n",
    "    #decoder\n",
    "    def decode(self,x):\n",
    "        ## WRITE ME\n",
    "        return y\n",
    "    \n",
    "    #forward propagation\n",
    "    def prop(self,x):\n",
    "        ## WRITE ME\n",
    "        return reconst_x\n",
    "    \n",
    "    #reconstruction error\n",
    "    def reconst_error(self,x,noise):\n",
    "        ## WRITE ME\n",
    "        return error, reconst_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Autoencoder(train_X.shape[1],500,T.nnet.sigmoid)\n",
    "\n",
    "x = T.matrix('x')\n",
    "noise = T.matrix('noise')\n",
    "\n",
    "cost,reconst_x = model.reconst_error(x,noise)\n",
    "params  = model.params\n",
    "gparams = T.grad(cost, params)\n",
    "updates = sgd(params,gparams) \n",
    "\n",
    "train = theano.function([x,noise], [cost,reconst_x], updates=updates, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corruption_level = 0.3\n",
    "batch_size = 100\n",
    "nbatches = train_X.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_X = shuffle(train_X)\n",
    "    err_all=[]\n",
    "    for i in range(0,nbatches):\n",
    "        start = i * batch_size\n",
    "        end   = start + batch_size\n",
    "        \n",
    "        noise = rng.binomial(size=train_X[start:end].shape, n=1, p=1-corruption_level)\n",
    "        err,reconst_x = train(train_X[start:end],noise)\n",
    "        err_all.append(err)\n",
    "    print \"Epoch:%d, Error:%lf\" %(epoch, np.mean(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重みの可視化\n",
    "* corruption_levelを変更して違いを観測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = model.W.get_value(borrow=True).T\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(100):\n",
    "    ax = fig.add_subplot(10, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(weight[i].reshape((28,28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題2：RBMの実装．また，MNISTを用いて次のことを確認．\n",
    "* reconstruction errorが小さくなっている（学習が進んでいる）．\n",
    "* 重みの可視化（特徴の可視化）．\n",
    "* 文字の生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1234)\n",
    "theano_rng = RandomStreams(rng.randint(1234))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBMクラスの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    def __init__(self,visible_dim,hidden_dim,function,k):\n",
    "        ## WRITE ME\n",
    "        self.a = \n",
    "        self.b = \n",
    "        \n",
    "        self.params = []\n",
    "        \n",
    "    def propup(self,input):\n",
    "        ## WRITE ME\n",
    "        return output\n",
    "    \n",
    "    def propdown(self,input):\n",
    "        ## WRITE ME\n",
    "        return output\n",
    "\n",
    "    #p(h|v)\n",
    "    def ph_v(self,v):\n",
    "        ## WRITE ME\n",
    "        return h_sample,h\n",
    "    \n",
    "    #p(v|h)\n",
    "    def pv_h(self,h):\n",
    "        ## WRITE ME\n",
    "        return v_sample,v\n",
    "    \n",
    "    #gibbs sampling(h→v→h)\n",
    "    def gibbs_hvh(self,h):\n",
    "        ## WRITE ME\n",
    "        return h_sample,mean_h\n",
    "        \n",
    "    #gibbs sampling(v→h→v)\n",
    "    def gibbs_vhv(self,v):\n",
    "        ## WRITE ME\n",
    "        return v_sample,mean_v\n",
    "    \n",
    "    #cost(free energy)\n",
    "    def free_energy(self,input):\n",
    "        y = T.dot(input,self.W) + self.b\n",
    "        return -T.dot(input,self.a) -T.sum(T.log(1 + T.exp(y)),axis=1)\n",
    "    \n",
    "    #reconstruction error\n",
    "    def reconst_error(self,v):\n",
    "        v_sample = v\n",
    "        for k in range(self.k):\n",
    "            v_sample,mean_v = model.gibbs_vhv(v_sample)\n",
    "            \n",
    "        cross_entropy = ## WRITE ME\n",
    "        return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "nbatches = train_X.shape[0] // batch_size\n",
    "\n",
    "persistent = train_X.copy()\n",
    "for epoch in range(10):\n",
    "    train_X,persistent = shuffle(train_X,persistent)\n",
    "    err_all=[]\n",
    "    for i in range(0,nbatches):\n",
    "        start = i * batch_size\n",
    "        end   = start + batch_size\n",
    "        \n",
    "        cost,_x = train(train_X[start:end],persistent[start:end])\n",
    "        persistent[start:end]=_x\n",
    "        err = reconst(train_X[start:end])\n",
    "        err_all.append(err)\n",
    "    print \"Epoch:%d, Error:%lf\" %(epoch, np.mean(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重みの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = model.W.get_value(borrow=True).T\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(100):\n",
    "    ax = fig.add_subplot(10, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(weight[i].reshape((28,28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gibbs sampling\n",
    "v = T.matrix(\"v\")\n",
    "[sample_v,mean_v],updates = theano.scan(fn=model.gibbs_vhv,outputs_info=[v,None],n_steps=1000)\n",
    "sample = theano.function([v], [sample_v[-1],mean_v[-1]],updates=updates,allow_input_downcast=True)\n",
    "\n",
    "#seed\n",
    "test_X = shuffle(test_X)\n",
    "sample_v,mean_v = sample(test_X[0:100])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(100):\n",
    "    ax = fig.add_subplot(10, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(mean_v[i].reshape((28,28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked auto-encoderの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from collections import OrderedDict\n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_x, mnist_y = mnist.data.astype(\"float32\")/255.0, mnist.target.astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のセルを完成させて提出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Autoencoder (or RBM)\n",
    "class Autoencoder:\n",
    "    def __init__(self,visible_dim,hidden_dim,W,function):\n",
    "        ## WRITE ME\n",
    "        \n",
    "    #encoder\n",
    "    def encode(self,x):\n",
    "        ## WRITE ME\n",
    "    \n",
    "    #decoder\n",
    "    def decode(self,x):\n",
    "        ## WRITE ME\n",
    "    \n",
    "    #forward propagation\n",
    "    def prop(self,x):\n",
    "        ## WRITE ME\n",
    "    \n",
    "    #reconstruction error\n",
    "    def reconst_error(self,x,noise):\n",
    "        ## WRITE ME\n",
    "        \n",
    "#SGD\n",
    "def sgd(params,gparams,lr=0.1):\n",
    "    updates = OrderedDict()\n",
    "    for param, gparam in zip(params, gparams):\n",
    "        updates[param] = param - lr * gparam\n",
    "    return updates\n",
    "    \n",
    "#Multi Layer Perceptron\n",
    "class Layer:\n",
    "    def __init__(self, in_dim, out_dim, function):\n",
    "        ## WRITE ME\n",
    "        self.set_pretraining()\n",
    "\n",
    "    def fprop(self, x):\n",
    "        ## WRITE ME\n",
    "    \n",
    "    def set_pretraining(self):\n",
    "        ae = Autoencoder(self.in_dim,self.out_dim,self.W,self.function)\n",
    "\n",
    "        x = T.matrix('x')\n",
    "        noise = T.matrix('noise')\n",
    "\n",
    "        cost,reconst_x = ae.reconst_error(x,noise)\n",
    "        params  = ae.params\n",
    "        gparams = T.grad(cost, params)\n",
    "        updates = sgd(params,gparams)\n",
    "\n",
    "        self.pretraining = theano.function([x,noise], [cost,reconst_x], updates=updates, allow_input_downcast=True)\n",
    "        \n",
    "        hidden = ae.encode(x)\n",
    "        self.encode_function = theano.function([x], hidden, allow_input_downcast=True)\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(mnist_x, mnist_y, test_size=0.2, random_state=42)\n",
    "\n",
    "layers = [## WRITE ME\n",
    "]\n",
    "\n",
    "#Pre-training\n",
    "X = train_X\n",
    "for l, layer in enumerate(layers[:-1]):\n",
    "    corruption_level = ## WRITE ME\n",
    "    batch_size = 100\n",
    "    nbatches = X.shape[0] // batch_size\n",
    "\n",
    "    for epoch in range(10):\n",
    "        ## WRITE ME\n",
    "        print \"Pre-training:: layer:%d, Epoch:%d, Error:%lf\" %(l,epoch, np.mean(err))\n",
    "    X = layer.encode_function(X)\n",
    "\n",
    "#Fine-tuning\n",
    "x, t = T.fmatrix(\"x\"), T.ivector(\"t\")\n",
    "params = []\n",
    "for i, layer in enumerate(layers):\n",
    "    params += layer.params\n",
    "    if i == 0:\n",
    "        layer_out = layer.fprop(x)\n",
    "    else:\n",
    "        layer_out = layer.fprop(layer_out)\n",
    "\n",
    "y = layers[-1].h\n",
    "cost = - T.mean((T.log(y))[T.arange(x.shape[0]), t])\n",
    "\n",
    "gparams = T.grad(cost, params)\n",
    "updates = sgd(params,gparams)\n",
    "\n",
    "train = theano.function([x,t], cost, updates=updates)\n",
    "valid  = theano.function([x,t],[cost, T.argmax(y, axis=1)])\n",
    "test  = theano.function([x],T.argmax(y, axis=1))\n",
    "\n",
    "batch_size = 100\n",
    "nbatches = train_X.shape[0]//batch_size\n",
    "for epoch in range(50):\n",
    "    ## WRITE ME\n",
    "    print \"EPOCH:: %i, Validation cost: %.3f, Validation F1: %.3f\"%(epoch+1, valid_cost, f1_score(valid_y, pred, average=\"macro\"))\n",
    "\n",
    "#pred_y = test(test_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
